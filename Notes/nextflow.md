<span style="font-size: 2em; font-weight: bold; color: teal;">Nextflow</span>

# Intro

Nextflow is a workflow language to make reproducible workflows at scale.

# Running a Workflow

## nextflow run

You can run a workflow using `nextflow run`.

```bash
nextflow run <script_name.nf>
```

## Nextflow Subcommands

Nextflow comes with lots of builtin **subcommands**. You can see what a subcommand does using `nextflow help`.

```bash
nextflow help <subcommand name>
```

### Options

Core **options** like `resume` or `version` have one `-`.

User defined options like `output` have two `--`.

* `version` - see version
* `resume` - resume using cached data
* `ansi-log` - if there are more than one task per process, set to `false` to see all of them not just the summary. (ANSI logging system writes the logs from multiple tasks to one line). However, this view doesn't show if the process was successful. 

https://www.nextflow.io/docs/latest/reference/cli.html 

## The work Directory

Each process can generate one or multiple **tasks**. These get a **task hash**. When Nextflow runs, it creates a directory called `work` which has subdirectories for the working directories of all the tasks. These contain the output files for each task. 

For each of these **task directories**, Nextflow:

* Stages input files
* executes the instructions
* writes outputs and log files

Each task is isolated from each other (so there are no collisions of files with the same name) and can run in parallel. Every time the nextflow pipeline is run (without resuming) a new directory in work is created. 

When a process receives a file input, like the output from the first process, it is soft linked into the workspace of the second in the `work` directory. This is called **staging**. This soft linking is to save space, isolate different tasks and because all file paths in the final bash script have paths relative to the working directory (subdirectory of `work`).

Hidden files:

* `.command.begin` - metadata related to the beginning of execution - sentinel file
* `.command.err` - `stderr`
* `.command.log` - `srderr` + `stdout`
* `.command.out` - `stdout`
* `.command.run` - what Nextflow executes - autogenerated
* `.command.sh` - the script generated from the process - useful for debugging
* `exitcode` - the exit code from the task

You can remove files from work using `nextflow clean`. The `-before` option can set the earliest run you want to keep. `-n` is used to tell you what will be removed without removing them. `-f` actually removes them.

```bash
nextflow clean -before run_name -f
```

## output

By default nextflow puts output in a `results` directory. This contains soft links to output in the task subdirectories. You can change this name using `-o newname` or `-output-dir newname` when running the pipeline.

## resume

Adding `-resume` to the nextflow run command means that the workflow will look for intermediate data in the `work` directory and use those. The task hash will be the same as the original run. This is useful if the pipeline fails or during development of a pipeline.

You can use `nextflow log` to see previous runs. This information can also be found in `history/.nextflow`. These have run names which can be used to resume specific runs.

```bash
nextflow run hello-world.nf -resume run_name
```

## Running From Remote Repos

You can run pipelines from remote repos, like GitHub, without manually downloading it first. The first time it is run, Nextflow downloads and chaches it locally. By default, Nextflow runs the latest version from the default branch. You can specify a version (tag), branch or commit using `-r`.

```bash
nextflow run <repo> -r v1.3
```

# Writing Workflows

## Processes

**Processes** hold a unit of logic for your workflow. It starts with the `process` keyword followed by the process name.

```groovy
process sayHello {

    input:
    val 'hello'
    
    output:
    path 'output.txt' // path qualifier says it should be handled as a path

    script:
    """
    echo 'Hello World!' > output.txt
    """
}
```

### input

`input` - expected input. You can have as many inputs as you like. When using them in the workflow, pass them as arguments in the exact same order.

```groovy
input:
path input_files
val batch_name
```

```groovy
myProcess(params.input_file, params.batch_name)
```

### output

`output` - tells what output to expect from the script. Required. Doesn't create the file, just used to verify the command has run correctly.

You can output more than one file and can give each output a tag using the `emit` keyword.

```groovy:
output:
path 'output1.txt', emit: out1
path 'output2.txt', emit: out2 
```

These can be used later e.g. in publish or main by referencing the name like an operator.

```groovy
publish:
first_output = myProcess.out.out1
second_output = myProcess.out.out2
```

They can also be reference by index, as the output is an array, but this is less reliable.

```groovy
publish:
first_output = myProcess.out[0]
second_output = myProcess.out[2]
```

### script

`script` - a piece of code. required. In """triple quotes""". By default it is bash but can do other languages with the relevant hashbang

You can put groovy script within the `script` block. This will run before the bash script is created and will not be included in the final bash script. You can do this to:

* Compute something
* Transform inputs
* Build filenames
* Add logic
* Prepare variables for the bash part

```groovy
script:

def message = greeting.toUpperCase()

"""
echo '${message}' > '${greeting}.txt'
"""
```

https://nextflow.io/docs/latest/process.html

## Workflows

**Workflows** orchestrates the processes. You can view the workflow by clicking `Preview DAG`. 

```groovy
workflow {
    main:
    sayHello() // what to run
    secondProcess(sayHello.out)

    publish: // what output is important
    first_output = sayHello.out
}
```
https://nextflow.io/docs/latest/workflow.html  

### main

`main` orchestrates the processes. Processes are run in the order the inputs become available, not the order you write them in `main`. If two processes have no dependency between them, they can run in parallel. Nextflow does this my creating a graph with the processes as nodes and the edges being the data between them. This is called a **dataflow model**.

### publish

`publish` - the publishes the results i.e. tells nextflow what output is important. By default this puts output in a `results` directory. This contains soft links to output in the task subdirectories. You can change this name using `-o newname` when running the pipeline.

https://nextflow.io/docs/latest/workflow.html#publishing-outputs 

## Output

This tells Nextflow where to put the **output** files. It contains options for the output:

* `mode` - change to `copy` to make a copy of the files instead of soft linking them. This means that you can delete the work directory with the intermediate files after.

```groovy
output {
    first_output {
        path 'hello_world' // create a subdirectory in results
        mode 'copy' // creates a copy of a file instead of a soft link
    }
}
```

You can also make them dynamic:

```groovy
output {
    first_output {
        path { sayHello.name } // .name gives the name of the process
        mode 'copy' // k
    }
}
```
## Variables

**Variables** are declared using the data type it is. 

* `val` - string, ints, floats, bools, lists, maps, tuples, objects
* `Path` - path to a file - tells Nextflow to stage the file before running

```groovy
process sayHello {
    input:
    val greeting

    output:
    path 'output.txt'

    script:
    """
    echo '${greeting}' > output.txt
    """
}
```

## params

You can tell nextflow what the input is in the workflow:

Hardcoding:

```groovy
workflow {
    main:
    sayHello('Hello') 
}
```

Command line:

```groovy
params {
    greeting: String = 'Hello' // optional data type, optional default value
}

```groovy
process sayHello {
    input:
    val greeting
}

```groovy
workflow {
    main:
    sayHello(params.greeting)
}
```

```bash
nextflow run hello-world.nf --greeting "Hello"
```
https://nextflow.io/docs/latest/config.html#workflow-parameters

## Syntax

### Quotes

Quotes are handled by both groovy and bash. In the example below:

```groovy
"""
echo '${greeting}' > '${greeting}-output.txt'
"""
```

1. Groovy creates a bash script
   * `"""..."""` - double quotes - groovy expands the variables inside
   * `{$greening}` - expanded by groovy
  
```bash
echo 'hello' > 'hello-output.txt'
```

2. bash runs the script
   * `'${greeting}'` - single quotes protect expanded variable from bash expansion

This means, to use bash variables, you need to escape the variable so it is ignored by groovy and turned into a literal string (removing the `\`).

```groovy
"""
var='greeting'
echo "\${var}" > "\${var}-output.txt" # echo "${var}" > "${var}-output.txt" 
"""
```

### Dynamic Closures

A **dynamic closure**  is a block of code that can be passed around like a variable it is dynamic because:

* It resolves at runtime 
* Can access variables
* Can change its behaviour with different inputs

For example:

```groovy
// greeting is a temporary variable
{ greeting -> "Before flatten: $greeting" }
```

## Channels

Often you'll want to run the same script with multiple inputs. Channels are streams that allow you to shuttle these inputs from one step to another in multi-step workflows in parallel in isolation. You create a channel at the beginning of the workflow. Each process consumes and creates channels automatically. 

Channels:

* Carry data between processes
* Trigger process execution when the data arrives
* Allow for data to be transformed in-flight
* Define pipeline dependencies
* Signal completion

 The `channel` object has everything to do with channels.

By convention, the name of a channel ends in `_ch`.

```groovy
workflow {
    main:
    // channel
    greeting_ch = channel
        .of('Hello', 'Bonjour', 'Hola')
        .view

    // pass the channel as an argument
    sayHello(greeting_ch)
}
```
https://nextflow.io/docs/latest/workflow.html

### Channel Factories

A **channel factory** is a type of **operator** that creates channels. Different channel factories create channels that emit different types of data like values or files. 

* `.of` - simplest way to create channels
* `.fromList` - create multiple tasks in a channel using an array `[]`.
* `.fromPath` - creates a channel from a file (takes a Path)


https://nextflow.io/docs/latest/reference/channel.html 

### Operators

The `channel` object has several **operators** that can add functionality. These happen in order like pipes in tidyverse.

* `.view()` - prints the contents of the channel to the terminal - used for debugging.
  * Can add a way to format the output

```groovy
greeting_ch = channel.of = channel
    .of('Hello!')
    .view()
```

* `.splitCsv()` - parses CSV files a row at a time
* `.flatten()` - unpacks an array and emits them as separate items

```groovy
greeting_ch = channel.fromPath(params.input)
    .splitCsv()
    .flattern()
```

* `.map()` - run a function over every element and do a transformation on it

```groovy
.map {row -> row[0]}
```

* `.out` - get the output
* `.collect()` - collects outputs

```groovy
workfow {
    main:
    ...
    collectGreetings(covertToUpper.out.collect())
}
```

https://nextflow.io/docs/latest/reference/operator.html 

## Modules

Modules are like methods. Instead of putting everything in `main` you can write them elsewhere an import them. These can be for each process. This means they can be shared between pipelines and collaborators through nf-core. By convention they are usually stored in `modules/`.

You then include them into your script using `include`.

```groovy
include { sayHello } from `./modules/sayHello.nf`
```

https://nextflow.io/docs/latest/module.html 

# Containers

You can use containers to run processes and Nextflow will:

* Pull the image
* Mount the data 
* Run the process inside the container
* Clean up the container after

## Using Docker

Download the image URI e.g. from wave. 

```bash
docker pull <URI>
```

Run the container.

```bash
docker run --rm <URI> <command>

# Interactive
docker run --rm --it <URI> /bin/bash
```

Mount a volume.

```bash
docker run --rm -v <local filesystem>:<location in the container>
```

## Add to a Script

You cn add containers to processes using the `container` keyword.

```groovy
process cowpy {
    container <URI>
}
```

You will also need to create a `nextflow.config` file. This is a file that, if it's in the working directory, it will be loaded automatically when the pipeline is launched. In this config file add:

```
docker.enabled = true
```

You can set a global container for all processes by including this in the config file:

```
process.container = <URI>
```

You can see how this is working by looking at the `.command.run` file in the `nxf_launch` function.

# Config Files

There are multiple ways config files can be managed. They can be stored in different places and are retrieved in an order of precedence:

1. `$NXF_HOME/config` (nextflow home directory) - always loaded by nextflow runs
2. `nextflow.config` in the project directory
3. `nextflow.config` in the launch directory e.g. a subdirectory (make sure to adjust file paths) - putting them here is good for testing
4. Config files specified with `-c <config-files>` - applied in the order they are specified (`-C` will override all)

Lower down ones override the upper ones.

Order of a config file matters because things specified later in the file override preceding things.

1. `docker.enabled = true`
2. process
3. params
4. profiles
5. `outputDir`
6. workflow  

You can check what the resolved configuration is using `nextflow config`.

```bash
nextflow config -profile my_laptop,test

## Parameters

### config files

Putting the parameters in the config allows people to change the default values more easily. The parameters is the same as in the script but with the type declaration and `:` removed. Values in config files override those in the script.

Config:

```groovy
params {
    greeting = 'Hello' 
}
```

You can then remove the default values from the script.

Script:

```groovy
params {
    greeting: String 
}
```

### Parameter Files

This is a reproducible way of providing parameters. They can be used by using `-params-file`. This is another good way of testing or getting someone else to run with specific parameters. 

```bash
nextflow run hello.nf -params-file <path to file>
```

**YAML**

```
input: "data/greetings.csv"
batch: "tux-batch"
character: "tux"
```

**JSON**

```
{
"input": "data/greetings.csv"
"batch": "tux-batch"
"character": "tux"
}
```

## Outputs

### CLI Option

You can change the directory results are published to from `results` to a different directory using `-outputDir` or `-o`.

```bash
nextflow run hello.nf -o <output dir name>
```

### Config

**Change output dir**

You can also do this using `outputDir` in the config. This also allows you to use variables.

config:

```groovy
outputDir = "path/${params.batch}"
```

Note: these will be overridden by `-o`. 

**Change mode**

You can also change the output mode.

```groovy
workflow {
    output.mode = 'copy'
}

//or

workflow.output.mode = 'copy'
```

## Environments

### Docker

You can specify Docker:

```groovy
docker.enabled = true
```

### Conda

You can also use conda. This creates a separate conda environment for each process which avoids clashes between different process dependencies. Nextflow will cache these environments. 

Config:

```groovy
conda.enabled = true
```

Script:

```groovy
process sayHello {
    conda 'conda-forge::cowpy==1.1.5' // channel::package==version
}
```

You can add both conda and a container platform so the user can choose. You can also use a mixture of conda for some processes and containers for others. If both are enabled, docker takes priority.

https://nextflow.io/docs/latest/conda.html 

## Platforms

Nextflow allows pipelines to be run on many different compute infrastructures. The choice of executor is set by the `executor` process directive. 

Executors:

* local (default)
* slurm - adds the `SBATCH` headers to the `command.run`

```groovy
process {
    executor = 'local'
}
```

This is done at the process level, so different processes can have different executors.

https://nextflow.io/docs/latest/executor.html 

## Compute Resources

Nextflow provides a standard syntax to provide HPC platforms with the right parameters. Nextflow takes these and generates the right scripts. They are specified using these process directives:

* `cpus` (default 1)
* `memory` (default 2.GB)
* `queue`

https://nextflow.io/docs/latest/reference/process.html#process-directives 

Config:

```groovy
process {
    executor = 'local'
    cpus = 1
    memory = 2.GB
}
```

If there are not enough resources for a process, Nextflow will wait until processes are finished, freeing up the resources needed. Different processes use different amounts of resources. You can specify resources for processes individually.

```groovy
process {
    executor = 'local'
    withName: 'cowpy'
        cpus = 1
        memory = 2.GB
}
```

Different executors will have different resource availability. You can specify the maximum resources for your system using `resourceLimits`.

```groovy
process {
    executor = 'local'
    resourceLimits = 
        memory: 10.GB,
        cpus: 4,
        time: 30.d
}
```

### Usage Report

Nextflow can generate a usage report that you can use to optimise the config `cpus` and `memory` parameters using `-with-report`.

```bash
nextflow run hello.nf -with-report report-name.html
```

It also saves metadata like the parameters used.

https://nextflow.io/docs/latest/reports.html 

## Profiles

Profiles allow you to have different configurations for different infrastructures.

### Compute Resources

```groovy
profiles {
    my_laptop {
        process.executor = 'local'
        docker.enabled = true
    }
    univ_hpc {
        process.executor = 'slurm'
        conda.enabled = true
        process.resourceLimits = [
            memory: 750.GB,
            cpus: 200,
            time: 30.d
        ]
    }
}
```

You can select which profile to use in the command:

```bash
nextflow run hello.ng -profile my_laptop
```

### Parameters

You can also include parameters instead of using a parameter file.

```groovy
profiles {
    test {
        params.<parameter1>
        params.<parameter2>
    }
}
```

You can combine more than one profile that will be added in order:

```bash
nextflow run hello.nf -profile my_laptop,test
```


https://nextflow.io/docs/latest/config.html


